[{"content":"Shefali Garg received her Masters in Intelligent Information Systems from Language Technologies Institute at Carnegie Mellon University, USA in 2019 with research focus on NLP and Speech. She completed her Bachelors in Computer Science from Birla Institute of Technology and Science, India in 2016. She is currently working at Google DeepMind where she specializes in developing and refining large multimodal AI models leveraging Large Language Models (LLMs) alongside techniques like Parameter Efficient Fine Tuning (PEFT), Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). Previously, as part of Google‚Äôs Speech Research Team, I contributed to building large-scale Automatic Speech Recognition (ASR) models, with a focus on domain adaptation, data minimization through unsupervised learning, parameter-efficient fine-tuning, speech personalization, contextualization, and bias mitigation.\u0026quot;\nIn parallel with her research, she writes about topics related to Artificial Intelligence and Machine Learning, and shares these resources on her website to make them accessible for everyone.\nIn her spare time, she loves playing sports, particularly basketball and badminton and also expressing herself through painting.\nAll opinions shared here are her own and don‚Äôt represent her employer.\n","date":"26 November 2024","permalink":"/about/","section":"Shefali Garg","summary":"","title":"About"},{"content":"As a Software Engineer working for GenAI organization at Google DeepMind, I specialize in developing and refining large multimodal AI models, leveraging Large Language Models (LLMs) alongside techniques like Parameter Efficient Fine Tuning (PEFT), Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). Previously, as part of Google‚Äôs Speech Research Team, I contributed to building large-scale Automatic Speech Recognition (ASR) models, with a focus on domain adaptation, data minimization through unsupervised learning, parameter-efficient fine-tuning, speech personalization, contextualization, and bias mitigation.\u0026quot;\nStrongest Areas: GenAI, AI, LLM, ASR, Deep Learning, Natural Language Processing, Speech, Machine Learning Data Structures and Algorithms\nPublications # Improving Speech Recognition for African American English with Audio Classification\nAuthors: Shefali Garg, Zhouyuan Huo, Khe Chai Sim, Suzan Schwartz, Mason Chua, Al√´na Aks√´nova, Tsendsuren Munkhdalai, Levi King, Darryl Wright, Zion Mengesha, Dongseong Hwang, Tara Sainath, Fran√ßoise Beaufays, Pedro Moreno Mengibar ICASSP 2024 - IEEE International Conference on Acoustics, Speech and Signal Processing, 2024\nLink to Paper\nLarge-scale ASR Domain Adaptation Using Self-and Semi-supervised Learning\nAuthors: Dongseong Hwang, Ananya Misra, Zhouyuan Huo, Nikhil Siddhartha, Shefali Garg, David Qiu, Khe Chai Sim, Trevor Strohman, Fran√ßoise Beaufays, Yanzhang He ICASSP 2022 - IEEE International Conference on Acoustics, Speech and Signal Processing, 2022\nLink to Paper\nA Comparison of Supervised and Unsupervised Pre-Training of End-to-End Models\nAuthors: A. Misra, D. Hwang, Z. Huo, S. Garg, N. Siddhartha, A. Narayanan, K.C. Sim Interspeech, 731-735, 2021\nLink to Paper\nUserLibri: A Dataset for ASR Personalization Using Only Text\nAuthors: Theresa Breiner, Swaroop Ramaswamy, Ehsan Variani, Shefali Garg, Rajiv Mathews, Khe Chai Sim, Kilol Gupta, Mingqing Chen, Lara McConnaughey Interspeech 2022\nLink to Paper\nPentagon at MEDIQA 2019: Multi-task Learning for Filtering and Re-ranking Answers Using Language Inference and Question Entailment\nAuthors: H. Pugaliya, K. Saxena, S. Garg, S. Shalini, P. Gupta, E. Nyberg, T. Mitamura ACL-BioNLP Workshop 2019 arXiv preprint arXiv:1907.01643, 2019\nLink to Paper\nIncremental Layer-wise Self-supervised Learning for Efficient Speech Domain Adaptation on Device\nAuthors: Zhouyuan Huo, Dongseong Hwang, Khe Chai Sim, Shefali Garg, Ananya Misra, Nikhil Siddhartha, Trevor Strohman, Fran√ßoise Beaufays arXiv preprint arXiv:2110.00155, 2021\nLink to Paper\nFor a detailed list of publications see my Google Scholar or ResearchGate profile. Press \u0026amp; media # Intern developing facial recognition app for Google Glass [link] Memberships \u0026amp; academic services # Reviewer for International Conference on Speech and Computer (SPECOM) 2024 Reviewer for Conference on Neural Information Processing Systems (NeurIPS) 2024 Reviewer for IEEE Spoken Language Technology Workshop (SLT) 2024 Member IEEE Signal Processing Society (SPS) Society Scholarships \u0026amp; Awards # Mitacs Globalink Research Internship by Mitacs, Canada in Mar 2015 Inspire Scholarship for Higher Education (S.H.E) : Issued by Department of Science and Technology (DST), Ministry of Science and Technology, Government of India in Aug 2011 National Talent Search Examination Scholar (NTSE) : Issued by National Council of Educational Research and Training in Mar 2009 ","date":"26 November 2024","permalink":"/research/","section":"Shefali Garg","summary":"","title":"Research"},{"content":"Hello, I\u0026rsquo;m Shefali. Welcome to my website üëã I work as a Software Engineer at Google Deepmind. Alongside my research, I enjoy sports, painting, coding and sharing insights through my articles. Learn more ","date":null,"permalink":"/","section":"Shefali Garg","summary":"","title":"Shefali Garg"},{"content":"","date":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI"},{"content":"","date":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM"},{"content":"","date":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"Topic 1: Transformers and Attention ‚Äì The Backbone of LLMs #Welcome to the first deep dive in the series! In this topic, we\u0026rsquo;ll explore Transformers and Attention, two concepts that form the core of modern Large Language Models (LLMs) like Gemini and GPT. But before we dive into this, it‚Äôs important to have a basic understanding of Machine Learning and Deep Learning concepts. If you\u0026rsquo;re new to these topics, don‚Äôt worry! You can check out these excellent courses to build foundational knowledge.\nCourse Recommendations\nMachine Learning Specialization (Stanford \u0026amp; DeepLearning.AI) | Coursera - A beginner-friendly, 3-course program by AI visionary Andrew Ng for fundamental AI and ML concepts. DeepLearning.AI‚Äôs Deep Learning Specialization on Coursera ‚Äì A comprehensive and Intermediate level- series to get you up to speed with the basics of deep learning with neural nets. What is Language Modeling? #Language modeling is the task of predicting the next word or sequence of words in a sentence based on the preceding context. It involves understanding the structure, grammar, and meaning of language, which enables a model to generate coherent and contextually appropriate text. Language models are trained on large corpora of text and learn the statistical relationships between words or tokens, which allows them to predict how likely a word is to appear next in a sequence.\nThe model generates text by considering previous words (or tokens) and using that context to generate the next word, phrase, or sentence. For example:\nContext: \u0026ldquo;The weather today is\u0026rdquo; Prediction: \u0026ldquo;sunny\u0026rdquo; (or any other word that fits based on the model‚Äôs training) Language models are widely used in many NLP tasks, Text Generation and Summarization, Machine Translation etc. Traditional language models, like n-gram models, used simple statistical methods to predict the next word based on the preceding ones. However, modern deep learning-based language models use more powerful neural networks to learn more complex patterns and relationships within the language.\nBefore Transformers, models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks were used for sequence processing. RNNs and LSTMs processed text one token at a time, maintaining a memory of previous tokens. However, they struggled with long-range dependencies, as their memory faded over long sequences, making it difficult to capture relationships in complex texts.\nReading Recommendations # Excellent Blog for understanding LSTMs: https://colah.github.io/posts/2015-08-Understanding-LSTMs/ Why LSTM/RNN Fail: https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0 Sequence to Sequence Models: Understanding Encoder-Decoder Sequence to Sequence Model | by Simeon Kostadinov | Towards Data Science Research Papers # [1409.3215] Sequence to Sequence Learning with Neural Networks The Transformer architecture solves this by utilizing attention mechanisms to enable parallel processing of data and capturing dependencies regardless of their distance in a sequence.\nWhat are Transformers And Attention ? #Transformers are the backbone of modern Natural Language Processing (NLP) and are the primary architecture behind LLMs. Introduced by Google in 2017 in the groundbreaking paper ‚ÄúAttention is All You Need‚Äù by Vaswani et al. (2017), Transformers revolutionized the way we process sequences of data, particularly for language modeling. The Transformer architecture was initially developed for translation tasks. It is a sequence-to-sequence model designed to convert sequences from one domain to another, such as translating French sentences into English.\nThe original Transformer architecture (shown in Figure 1) consists of two main components: the encoder and the decoder. The encoder processes the input text (e.g., a French sentence) and transforms it into a numerical representation. This representation is then passed to the decoder, which generates the output text (e.g., an English translation) in an autoregressive manner, meaning one word is predicted at a time based on the previously generated words.\nFigure 1 : Original Transformer Architecture (Ref: https://arxiv.org/abs/1706.03762)\nDon‚Äôt worry about the complex diagram above, and I don‚Äôt expect you to be familiar with terms like multi-head attention etc., just yet. :). I‚Äôll break down the key components of the Transformer for you and share the best resources to explore them in depth.\nThese are the key components of a transformer model.\nInput Embedding/Output Embedding:\nInput embeddings are used to represent the input tokens (e.g., words or subwords) provided to the model, while output embeddings represent the tokens predicted by the model. For instance, in a machine translation task, the input embeddings correspond to words in the source language, and the output embeddings correspond to words in the target language. Positional Encoding:\nSince Transformers don‚Äôt process data sequentially (like RNNs), positional encoding is used to add information about the position of words in a sentence so that the model can understand the order of words. Attention: This is the place where the magic happens !\nAttention allows a model to focus on the most relevant parts of the input while processing a sequence. This is crucial for understanding language because words can have relationships that extend over long distances. For example, in the sentence ‚ÄúThe cat that chased the mouse was very fast‚Äù, attention helps the model understand that ‚Äúcat‚Äù and ‚Äúwas‚Äù are related, even though they are far apart. The primary type of attention in Transformers is self-attention, which allows each word (or token) in a sequence to ‚Äúattend‚Äù to all the other words in the sequence when making predictions. This capability enables the Transformer to capture contextual relationships at scale.\nFigure 2 : One word \u0026ldquo;attends\u0026rdquo; to other words in the same sentence differently.\nRef: https://lilianweng.github.io/posts/2018-06-24-attention/ Multi-Head Attention: Instead of having a single attention mechanism, multi-head attention uses multiple attention layers to learn different relationships in the data simultaneously. This allows the model to capture more diverse patterns. Masked Multi-Head Attention: We force the model to only calculate attention corresponding to only the previous tokens that it has seen in the sentence and not for the future tokens. Used in the Decoder part of the transformer. Blogs explaining attention in detail:\nVisualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) ‚Äì Jay Alammar\nAttention? Attention! | Lil\u0026rsquo;Log\nFeed-Forward Layers:\nAfter the attention layer, a feed-forward network is applied to transform the representations for each token. This adds non-linearity and complexity to the model. Add \u0026amp; Norm Layers :\nEvery transformer layer consisting of a multi-head attention module and a feed-forward layer also employs layer normalization and residual connections. This is corresponding to the Add and Norm layer in Figure 1, where ‚ÄòAdd‚Äô corresponds to the residual connection and ‚ÄòNorm‚Äô corresponds to layer normalization. Output Layer:\nAn Output Layer, such as the Softmax layer, produces the final predictions of the model,converting the computed values into probabilities for tasks like classification or generating text. Reading Recommendations # I highly encourage this blog to anyone who wants to understand transformers and attention in the most simplified manner. It beautifully explains all the above concepts visually. The Illustrated Transformer ‚Äì Jay Alammar Youtube Video for the same: https://www.youtube.com/watch?v=-QH8fRhqFHM Transformer Paper: [1706.03762] Attention Is All You Need Hugging Face‚Äôs Course on Transformers ‚Äì Hands-on tutorials with practical implementation examples. Other References/Resources # Residual Networks: [1512.03385] Deep Residual Learning for Image Recognition Layer Normalization: [1607.06450] Layer Normalization Wrapping Up # Transformers revolutionized large language models (LLMs) and AI with the introduction of the self-attention mechanism, allowing models to effectively capture long-range dependencies and contextual relationships across entire sequences. Unlike RNNs and LSTMs, Transformers process data in parallel, significantly enhancing scalability and training speed. The architecture supports massive pretraining on diverse datasets, followed by fine-tuning for specific tasks, enabling powerful transfer learning capabilities. This scalability and flexibility have driven breakthroughs in performance, enabling the development of state-of-the-art LLMs like GPT and BERT. These models excel in a variety of tasks, including text generation, translation, and summarization. I hope you enjoyed the article and found the insights on transformers and their impact on AI and large language models helpful. If you have any questions or would like to explore the topic further, feel free to reach out!\nHappy Reading! :D\n","date":"25 November 2024","permalink":"/posts/tranformer/","section":"Posts","summary":"","title":"Transformers and Attention: The Backbone of LLMs"},{"content":"","date":null,"permalink":"/tags/self-supervised-training/","section":"Tags","summary":"","title":"Self-Supervised Training"},{"content":"","date":null,"permalink":"/tags/speech/","section":"Tags","summary":"","title":"Speech"},{"content":"Have you ever wished your voice assistant understood you a little better? Automatic speech recognition (ASR) technology is constantly evolving, but achieving high accuracy can be complex. Today, we‚Äôll explore a new approach called BEST-RQ (BERT-based Speech pre-Training with Random-projection Quantizer) introduced by Google Researchers in 2023 that has made waves in the field.\nThis innovative technique leverages the power of BERT, a popular language model, specifically adapted for speech recognition. But BEST-RQ takes it a step further by introducing a unique scalable pre-training method, allowing the model to learn effectively from vast amounts of unlabelled data, leading to significant improvements in accuracy.\nIntrigued? In this short blog post, we‚Äôll break down the key concepts behind BEST-RQ and how it‚Äôs transforming speech recognition. We‚Äôll explore how it simplifies the learning process and unlocks the potential for even more accurate and robust ASR systems in the future!\nProblem #Speech recognition aims to convert spoken language into written text. Traditional methods rely on labeled data, where spoken words are paired with their corresponding written text. But this approach requires a vast amount of labeled data, which can be expensive and time-consuming to collect.\nOne common design principle of self-supervised learning for speech recognition centers around learning representations. Inspired by the success of BERT (Devlin et al., 2018), a powerful technique in natural language processing, one research trend in the speech community is to build BERT-inspired algorithms.\nBERT is not directly applicable to speech because speech lacks these discrete tokens. We can‚Äôt directly feed the continuous speech signal into BERT, hence a need to bridge the gap between continuous speech signals and the discrete text tokens, and a solution for addressing this issue is through learning speech representation.\nWhile speech representation learning is crucial, integrating it with self-supervised learning presents two challenges.\nModel Architecture Limitation\nThe model must excel at both representation and downstream tasks, but optimal representation learning might not translate to efficient downstream processing (e.g., accessing future context for representation vs. low-latency requirements for recognition). Increased Design Complexity\nThe misaligned objectives and intricate design process for these combined algorithms can hinder research progress, potentially favoring complex solutions over simpler alternatives. The core problem BEST-RQ tackles is: how can we train a speech recognition model effectively without relying on a large amount of labeled data or needing to break speech down into discrete units like words, and at the same time keeping it all ‚ÄúSIMPLE‚Äù to be able to scale to larger new speech model architectures?\nOccam‚Äôs Razor for ML: Simplest solution often wins [Ref. Image]\nBEST-RQ‚Äôs Solution #BEST-RQ tackles the above problem by offering a compelling alternative. It introduces a novel technique of self-supervised training using a combination of Random Projection Quantizer (RPQ) and Masked Language Modeling (MLM).\nFig 1: Overview of BEST-RQ. The approach applies random projections to project the input speech signals to a randomly initialized codebook, and map them to discrete labels through finding the nearest vector in the codebook. The pre-training objective is for the ASR encoder to take the masked input signals and predict the labels corresponding to the masked part provided by the random-projection quantizer. Figure taken from Ref. [1]\nRandom Projection Quantizer (RPQ) #This is the heart of BEST-RQ and is the core innovation that bridges the gap between continuous speech and the discrete world BERT thrives in. RPQ has two key components - Projection matrix and Codebook both randomly initialized and not updated during training.\nProjection matrix #This projects the speech features (numerical representation of speech) into a lower dimension. The framework is described in Figure 1. This matrix is of size d x k, where:\nd is the dimensionality of the original speech features (typically high, like hundreds or thousands). k is the target dimensionality after projection (usually much lower than d). Codebook # To put simply, this is a collection of n code vectors, each of size k. These vectors represent the discrete code space. The size n of the codebook is a hyper parameter that can be tuned based on the specific task and dataset. The projection matrix A use Xavier initialization (Glorot \u0026amp; Bengio, 2010) and the codebook C use standard normal distribution for initialization, and the parameters are fixed during the pre-training process and therefore the quantizations are consistent during training.\nGiven an input vector x where x is a d-dimensional vector computed from speech signals, the random-projection quantizer maps x to discrete labels y through\nwhere:\n* A denotes a randomly initialized h √ó d matrix\n* C = {c1, ‚Ä¶, cn} is a set of randomly initialized hi-dimensional vectors\n* norml2() is a function that normalize the vector to have unit L2 norm.\nThe input data is normalized to have 0 mean and standard deviation of 1. The normalization is critical for pre- Self-supervised Learning with Random-projection Quantizer for Speech Recognition preventing the random projection to collapse to a small subset of codes.\nAs shown in Figure 1, The BEST- RQ algorithm masks speech signals (explained below) and feeds them to the encoder part of the speech recognition model.\nThe encoder learns to predict the masked region based on the unmasked speech signals where the learning targets are labels provided by RPQ.\nThe RPQ projects speech signals to a randomly initialized matrix, and finds a nearest vector in a randomly initialized codebook. The index of that vector is the target label.\nMasked Language Modeling (MLM)\nSimilar to how BERT works with text, BEST-RQ uses MLM for training. Speech segments are masked (replaced with silence or noise).\nThe approach applies masks directly on the speech signal, where the masking strategy samples at every frame whether to apply masks with a fixed probability. Each mask spans from the starting frame with a fixed length. The masked parts are replaced with a noise sampled from a normal distribution with 0 mean and 0.1 standard deviation.\nThe model, typically a Transformer architecture, is tasked with predicting the masked parts based on the surrounding context. Instead of predicting words like BERT, the model predicts the labels (codebook indices) of the masked speech using the RPQ predictions as targets.\nThe pre-training process adds a softmax layer on top of the ASR encoder to learn to predict the quantized speech labels.\nKey Point: Frozen and Independent RPQ\nUnlike other self-supervised methods, RPQ (projection matrix and codebook) is randomly initialized and not trained during the process. This removes the need for the model to learn the intricacies of the codebook, allowing it to focus solely on capturing meaningful speech representations.\nSince the random-projection quantizer is independent of the ASR encoder, the pre-training is flexible and can work with different architectures of the ASR encoder.\nBenefits of this approach # Performance\nBEST-RQ achieves competitive results compared to other methods, even with lower latency for real-time applications.\nThis approach shows similar WERs as the existing state-of-the-art results on LibriSpeech with non-streaming models, and outperform wav2vec 2.0 and w2vBERT on LibriSpeech with streaming models and on multilingual tasks with non-streaming models.\nOn multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT. Flexibility\nThe model architecture is independent of the RPQ design. Neither the matrix nor the codebook is updated during self-supervised learning. Since the random-projection quantizer is not trained and\nis separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. Simplicity and Focus on Core Task\nBy avoiding complex representation learning, BEST-RQ spends less time grappling with representation learning and can concentrate on the core task ‚Äî predicting the masked parts of the speech using the provided codebook labels. Here‚Äôs an analogy: Imagine you‚Äôre lost in a giant forest (high-dimensional speech data) and need to find a specific landmark (desired speech representation).\nTraditional methods: You meticulously learn the entire forest layout (complex representation learning) to navigate and find the landmark.\nBEST-RQ approach: You‚Äôre given a helicopter (random projection matrix) that takes you high above the forest (lower dimension) and a pre-defined map (codebook) with landmarks marked. You simply find the location that looks most similar to your current view (nearest codebook vector) ‚Äî faster and with less effort!\nFurther Exploration # Research is ongoing to understand how well RPQ captures speech information compared to learned quantizers. Exploring different types of quantizers and experiment designs might yield further improvements. Thank you very much for reading this and I hope I was able to clarify a few notions to the people who are just starting to get into Speech Models Pre-training World. Please feel free to suggest edits if you find any mistakes !\nReferences # Self-Supervised Learning with Random-Projection Quantizer for Speech Recognition by Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu \u0026amp; Yonghui Wu. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. Attention Is All You Need by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin Understanding the difficulty of training deep feedforward neural networks by Xavier Glorot \u0026amp; Yoshua Bengio wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training, Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, Yonghui Wu ","date":"6 April 2024","permalink":"/posts/bestrq/","section":"Posts","summary":"","title":"Understanding BEST-RQ: A Simple Self-Supervised Pre-training Approach for Speech Recognition"},{"content":" Title: The Answer To Everything :)Medium: Acrylic\nTitle: Wear Your CrownMedium: Acrylic\nTitle: The Beginnings With Lord GaneshaMedium: Acrylic\nTitle: It Doesn‚Äôt Always Have To Make Sense!Medium: Acrylic\nTitle: DreamMedium: Ink\nTitle: FriendsMedium: Acrylic\nTitle: The GirlMedium: Acrylic\nTitle: The FirstMedium: Charcoal\nTitle: WalkMedium: Acrylic\n","date":"1 January 2024","permalink":"/meta/art/","section":"Meta","summary":"","title":"Let's Paint a Little :)"},{"content":"","date":null,"permalink":"/meta/","section":"Meta","summary":"","title":"Meta"}]