[{"content":"","date":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI"},{"content":"","date":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM"},{"content":"","date":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"Hello, I\u0026rsquo;m Shefali. Welcome to my website üëã I work as a Software Engineer at Google Deepmind. Alongside my research, I enjoy sports, painting, coding and sharing insights through my articles. Learn more ","date":null,"permalink":"/","section":"Shefali Garg","summary":"","title":"Shefali Garg"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"The success of large language models (LLMs) like Gemini, GPT-4 etc is not only due to their massive scale but also the sophisticated training strategies employed. In the last post, we discussed about Transformers, the backbone of Large Language Models. In this post we\u0026rsquo;ll talk about these training strategies.\nTraining an LLM is a multi-phase process, typically consisting of pre-training, followed by various methods of fine-tuning to adapt the model for specific tasks. Over time, new techniques such as Supervised Fine-Tuning (SFT), Parameter-Efficient Fine-Tuning (PEFT), and Reinforcement Learning from Human Feedback (RLHF) have emerged to address the need for more efficient, scalable, and human-aligned approaches to model training. Figure 1: Broad Stages of Model Training\nImage Ref: Blog Pre-training: Learning General Language Representation #Pre-training is the first and most computationally expensive phase of training an LLM. During this phase, the model is trained on vast amounts of raw text data from diverse sources (books, websites, Wikipedia, news articles, etc.). The objective here is not to perform any specific task but to learn general patterns, syntax, semantics, and structure of language.\nKey Details: # Unsupervised Learning: Pre-training is typically done in an unsupervised manner. The model learns by predicting the next word (or token) in a sentence given the previous words. This is done using an objective called masked language modeling (MLM) or autoregressive language modeling. MLM (BERT- style): Random words in a sentence are masked, and the model learns to predict these masked words based on the context. Autoregressive (GPT - style): The model generates text token by token, predicting the next token in a sequence, conditioned on the previous tokens. Training Objective: The goal of pre-training is to optimize the model to generate meaningful text sequences, thereby learning intricate relationships between words and concepts. This phase often uses a cross-entropy loss between the predicted and actual tokens. Scalability: Pre-training relies on enormous datasets and is computationally intensive, requiring vast amounts of data and computing power (often thousands of GPUs or TPUs over weeks or months). As the model grows in scale, the data and computational requirements also scale up. Benefits: # General Knowledge: The pre-trained model accumulates a wealth of knowledge about language structure, common facts, and even world knowledge. However, it is still a general-purpose model and does not specialize in any particular domain or task. Transfer Learning: This phase allows the model to be later adapted to specific tasks through fine-tuning, enabling the transfer of learned representations to a variety of downstream applications like question answering, text generation, and sentiment analysis. Recommended Reading: # BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin et al. (). This paper introduces BERT, a popular transformer model, and provides the foundational ideas behind pre-training and fine-tuning for language tasks. Improving Language Understanding by Generative Pre-Training by Open AI Blog for Understanding GPT2: https://jalammar.github.io/illustrated-gpt2/ Blog for Understanding GPT3: https://jalammar.github.io/how-gpt3-works-visualizations-animations/ Supervised Fine-tuning (SFT): Adapting the Model for Specific Tasks #Fine-tuning is the second phase, where a pre-trained model is adapted to perform well on specific tasks. Unlike pre-training, which is performed on generic, unstructured data, fine-tuning uses task-specific datasets (e.g., a dataset for text classification, named entity recognition, etc.).\nThe model, having already learned general language patterns, now fine-tunes its parameters to optimize its performance on these specialized tasks.\nSupervised fine-tuning (SFT) takes the pre-trained model and adjusts its weights using a labeled dataset for a particular downstream task. This is the most straightforward and traditional form of fine-tuning.\nKey Details: # Objective: In SFT, the model\u0026rsquo;s behavior is tailored to fit a specific objective by minimizing the error between the model\u0026rsquo;s predictions and the true labels for a supervised task using gradient-based optimization algorithms (commonly stochastic gradient descent or its variants such as Adam). Examples of such tasks include text classification, named entity recognition, or question answering. Parameters: During SFT, the entire model, or selected layers, is updated based on the gradients computed from the error between the predicted output and the ground truth. Loss Function: During SFT, the model typically optimizes for a cross-entropy loss for classification tasks or mean squared error for regression tasks. This enables the model to specialize in the particular task at hand. Data Requirements: Fine-tuning typically requires a labeled dataset, often much smaller in size compared to the massive corpus used for pre-training. However, fine-tuning on smaller datasets can still yield significant performance gains for specific applications. Limitations: Although SFT is powerful, it can be data-intensive and may not always provide optimal performance for tasks with small or limited datasets. Additionally, it can be time-consuming to train large models with millions or billions of parameters. Recommend Reading: # Language Models are Few-Shot Learners by Tom B. Brown et al. This paper introduces GPT-3 and demonstrates how supervised fine-tuning allows LLMs to perform well in a few-shot learning setup. To address the above mentioned limitations, more efficient fine-tuning strategies have been developed, such as Parameter-Efficient Fine-Tuning (PEFT) and Reinforcement Learning from Human Feedback (RLHF).\nParameter-Efficient Fine-Tuning (PEFT): A More Efficient Approach #As the cost of training large models continues to increase, PEFT offers a more efficient alternative to traditional fine-tuning methods by focusing on modifying smaller components of a model (e.g., specific attention heads, embeddings, or even the prompt structure) instead of updating the entire network.\nThe central idea is to retain the majority of the pre-trained model‚Äôs parameters frozen (i.e., not updated during training) and only adjust a minimal set of parameters that are critical for the specific downstream task.\nTechniques in PEFT: # Adapters: Adapters are small modules inserted between layers of a pre-trained model. Figure below shows the different ways that the adapter layer can be inserted. These modules are trainable, while the rest of the model\u0026rsquo;s parameters are kept frozen. This allows the model to learn task-specific representations while minimizing the number of parameters that require optimization. Figure 2: Illustration of 3 representative adapter-based finetuning algorithms. Blue represents frozen, while yellow represents trainable parameters.\nImage Ref: arXiv Paper LoRA (Low-Rank Adaptation): LoRA proposes modifying only the low-rank matrices in the model, while keeping the original weights frozen. LoRA allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers‚Äô change during adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 2.,This approach effectively reduces the number of trainable parameters by approximating weight updates with low-rank matrix decompositions, making it more computationally efficient.\nFigure 3: The weight matrix is reparametrized into smaller matrices A and B. Only A and B are updated during training.\nImage Ref: arXiv Paper Prefix Tuning: This method introduces trainable vectors (prefixes) as shown in Figure 4 that are prepended to the model\u0026rsquo;s input tokens. The model then learns to adjust these prefixes for specific tasks, enabling task adaptation with minimal parameter updates.\nFigure 4: Fine-tuning (top) updates all Transformer parameters (the red Transformer box) and requires storing a full model copy for each task. Prefix-tuning (bottom), freezes the Transformer parameters and only optimizes the prefix (the red prefix blocks).\nImage Ref: arXiv Paper Prompt Tuning: Similar to prefix tuning, this approach involves learning task-specific \u0026ldquo;prompts\u0026rdquo; that are combined with the input data, allowing the model to adjust its response without modifying the model‚Äôs underlying parameters Key Details: # Methodology: PEFT focuses on optimizing a small number of model parameters or components that influence the model\u0026rsquo;s behavior the most. This allows the model to adapt to specific tasks with fewer parameters, drastically reducing the computational cost of fine-tuning. Efficiency: Rather than re-training the entire model, PEFT often focuses on adjusting specific prompts or lightweight modules, which allows large language models to be more adaptable and resource-efficient. Few-shot and Zero-shot Learning: PEFT is particularly useful in few-shot or zero-shot learning settings, where task-specific data is limited. By fine-tuning only a small portion of the model, PEFT can quickly adapt the model to new tasks using very few examples. Benefits: Significantly reduced memory requirements and faster training times compared to full fine-tuning. Additionally, PEFT methods often achieve competitive performance with far fewer parameters being updated, making them highly suitable for resource-constrained environments. Limitations: PEFT methods often require careful tuning of the low-rank or adapter parameters to ensure the quality of the learned task-specific representations. Furthermore, some techniques, like prefix or prompt tuning, may have limitations in modeling more complex task-specific nuances that require deeper adaptation of the model‚Äôs weights. Recommended Reading: # Adapter Paper : [1902.00751] Parameter-Efficient Transfer Learning for NL LORA Paper : [2106.09685] LoRA: Low-Rank Adaptation of Large Language Models Prefix Tuning Paper: [2101.00190] Prefix-Tuning: Optimizing Continuous Prompts for Generation Hugging Face Prefix Tuning Blog: Prefix tuning Hugging Face Prompt Tuning Blog: Prompt tuning Prompt Tuning Paper: [2104.08691] The Power of Scale for Parameter-Efficient Prompt Tuning Reinforcement Learning from Human Feedback (RLHF): Aligning with Human Preferences #Reinforcement Learning from Human Feedback (RLHF) is an advanced fine-tuning strategy that goes beyond traditional supervised learning by incorporating human feedback directly into the training process. RLHF has become increasingly important in domains where alignment with human values, preferences, or ethical standards is crucial.\nRLHF Process #As shown in the Figure 5, the process of RLHF typically involves the following steps:\nFigure 5: An example of RLHF procedure\nImage Ref: Newwhitepaper_Foundational Large Language models \u0026 text generation.pdf Pre-training: A large pre-trained LLM is first trained on a broad corpus of text (using unsupervised or supervised learning). Reward Model Training: In parallel, human evaluators provide feedback on model outputs, often through ranking responses or assigning scores based on specific criteria such as relevance, coherence, or safety. These feedback signals are used to train a reward model, which learns to predict the quality of model outputs. Reinforcement Learning (RL): The pre-trained model is then fine-tuned using reinforcement learning algorithms (such as Proximal Policy Optimization or PPO). The model generates responses to various prompts, and the reward model provides feedback on these outputs. The model\u0026rsquo;s parameters are updated based on the rewards, guiding the system toward producing higher-quality outputs over time. Key Details: # Human-in-the-loop: RLHF integrates human feedback into the reinforcement learning framework, where human evaluators provide feedback on model outputs in the form of rankings, preferences, or even direct corrections. This feedback guides the model toward more human-aligned behavior.\nReinforcement Learning: In RLHF, the model is treated as an agent interacting with an environment, where the feedback (often in the form of a reward signal) serves as the environment‚Äôs response to the agent‚Äôs actions. The model adjusts its policies (i.e., behavior) to maximize the expected reward, which is designed to reflect human preferences.\nApplications and Benefits: It has shown significant success in aligning models with human preferences, enhancing aspects like factual accuracy, coherence, and safety in generated content. One of the primary advantages of RLHF is its ability to optimize for complex, subjective metrics, which are difficult to capture using traditional supervised learning.\nChallenges: RLHF introduces several challenges, particularly around the quality and consistency of human feedback. Human annotators may introduce biases, and there is also the problem of reward model overfitting. Furthermore, RL algorithms often require substantial computational resources, especially when fine-tuning models with large numbers of parameters.\nDespite these challenges, RLHF has proven effective in fine-tuning models making them more useful in real-world applications like chatbots, content generation, and recommendation systems, where human-like reasoning and alignment are necessary.\nRecommended Reading: # Deep Reinforcement Learning from Human Preferences by Christiano et al.\nThis seminal paper describes the RLHF approach and how reinforcement learning can be used to train agents based on human feedback. Learning to Summarize with Human Feedback by Nisan Stiennon et al.\nThis paper explores how RLHF can be used for text summarization tasks, demonstrating its effectiveness in aligning model outputs with human preferences. Another interesting blog on RLHF, which explains the topic in detail: RLHF: Reinforcement Learning from Human Feedback Comparison of Pretraining, SFT, PEFT, and RLHF # Aspect Pretraining SFT (Supervised Fine-Tuning) PEFT (Parameter-Efficient Fine-Tuning) RLHF (Reinforcement Learning with Human Feedback) Purpose Learn general language patterns. Specialize for specific tasks. Fine-tune efficiently with fewer parameters. Align with human preferences. Data Type Large unlabeled text data. Labeled task-specific data. Labeled data + minimal task-specific changes. Labeled data + human feedback. Learning Mechanism Self-supervised (e.g., MLM, next token). Supervised learning on labeled data. Fine-tune small parameters, freeze others. Reinforcement learning with human feedback as reward. Training Goal Generalize across tasks. Optimize for specific tasks. Reduce fine-tuning costs and resource use. Align model behavior with human feedback. Pros Scalable, generalizes well. High performance on specific tasks. Fast, efficient fine-tuning. Aligns with human values. Cons Expensive, not task-specific. Requires large labeled datasets. May not achieve best performance. Expensive, depends on quality feedback. Conclusion #Training large language models is a complex, multi-stage process that involves both pre-training on vast, generic datasets and fine-tuning on specific tasks. The traditional approach to fine-tuning, Supervised Fine-Tuning (SFT), has been highly effective but is computationally expensive. Newer techniques like Parameter-Efficient Fine-Tuning (PEFT) provide more efficient alternatives for adapting models with fewer parameters and less data. Finally, Reinforcement Learning from Human Feedback (RLHF) goes one step further by incorporating human preferences into the training process, ensuring that models align with human values and expectations.\nI hope you enjoyed the article and found it helpful. If you have any questions or would like to explore the topic further, feel free to reach out! Happy Reading! :D\n","date":"12 December 2024","permalink":"/posts/training/","section":"Posts","summary":"","title":"Training Large Language Models: From Pre-training to Fine-tuning and Beyond ‚Äì SFT, PEFT, and RLHF"},{"content":"Shefali Garg received her Masters in Intelligent Information Systems from Language Technologies Institute at Carnegie Mellon University, USA in 2019 with research focus on NLP and Speech. She completed her Bachelors in Computer Science from Birla Institute of Technology and Science, India in 2016. She is currently working at Google DeepMind where she specializes in developing and refining large multimodal AI models leveraging Large Language Models (LLMs) alongside techniques like Parameter Efficient Fine Tuning (PEFT), Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). Previously, as part of Google‚Äôs Speech Research Team, I contributed to building large-scale Automatic Speech Recognition (ASR) models, with a focus on domain adaptation, data minimization through unsupervised learning, parameter-efficient fine-tuning, speech personalization, contextualization, and bias mitigation.\u0026quot;\nIn parallel with her research, she writes about topics related to Artificial Intelligence and Machine Learning, and shares these resources on her website to make them accessible for everyone.\nIn her spare time, she loves playing sports, particularly basketball and badminton and also expressing herself through painting.\nAll opinions shared here are her own and don‚Äôt represent her employer.\n","date":"26 November 2024","permalink":"/about/","section":"Shefali Garg","summary":"","title":"About"},{"content":"As a Software Engineer working for GenAI organization at Google DeepMind, I specialize in developing and refining large multimodal AI models, leveraging Large Language Models (LLMs) alongside techniques like Parameter Efficient Fine Tuning (PEFT), Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). Previously, as part of Google‚Äôs Speech Research Team, I contributed to building large-scale Automatic Speech Recognition (ASR) models, with a focus on domain adaptation, data minimization through unsupervised learning, parameter-efficient fine-tuning, speech personalization, contextualization, and bias mitigation.\u0026quot;\nStrongest Areas: GenAI, AI, LLM, ASR, Deep Learning, Natural Language Processing, Speech, Machine Learning Data Structures and Algorithms\nPublications # Improving Speech Recognition for African American English with Audio Classification\nAuthors: Shefali Garg, Zhouyuan Huo, Khe Chai Sim, Suzan Schwartz, Mason Chua, Al√´na Aks√´nova, Tsendsuren Munkhdalai, Levi King, Darryl Wright, Zion Mengesha, Dongseong Hwang, Tara Sainath, Fran√ßoise Beaufays, Pedro Moreno Mengibar ICASSP 2024 - IEEE International Conference on Acoustics, Speech and Signal Processing, 2024\nLink to Paper\nLarge-scale ASR Domain Adaptation Using Self-and Semi-supervised Learning\nAuthors: Dongseong Hwang, Ananya Misra, Zhouyuan Huo, Nikhil Siddhartha, Shefali Garg, David Qiu, Khe Chai Sim, Trevor Strohman, Fran√ßoise Beaufays, Yanzhang He ICASSP 2022 - IEEE International Conference on Acoustics, Speech and Signal Processing, 2022\nLink to Paper\nA Comparison of Supervised and Unsupervised Pre-Training of End-to-End Models\nAuthors: A. Misra, D. Hwang, Z. Huo, S. Garg, N. Siddhartha, A. Narayanan, K.C. Sim Interspeech, 731-735, 2021\nLink to Paper\nUserLibri: A Dataset for ASR Personalization Using Only Text\nAuthors: Theresa Breiner, Swaroop Ramaswamy, Ehsan Variani, Shefali Garg, Rajiv Mathews, Khe Chai Sim, Kilol Gupta, Mingqing Chen, Lara McConnaughey Interspeech 2022\nLink to Paper\nPentagon at MEDIQA 2019: Multi-task Learning for Filtering and Re-ranking Answers Using Language Inference and Question Entailment\nAuthors: H. Pugaliya, K. Saxena, S. Garg, S. Shalini, P. Gupta, E. Nyberg, T. Mitamura ACL-BioNLP Workshop 2019 arXiv preprint arXiv:1907.01643, 2019\nLink to Paper\nIncremental Layer-wise Self-supervised Learning for Efficient Speech Domain Adaptation on Device\nAuthors: Zhouyuan Huo, Dongseong Hwang, Khe Chai Sim, Shefali Garg, Ananya Misra, Nikhil Siddhartha, Trevor Strohman, Fran√ßoise Beaufays arXiv preprint arXiv:2110.00155, 2021\nLink to Paper\nFor a detailed list of publications see my Google Scholar or ResearchGate profile. Press \u0026amp; media # Intern developing facial recognition app for Google Glass [link] Memberships \u0026amp; academic services # Reviewer for International Conference on Speech and Computer (SPECOM) 2024 Reviewer for Conference on Neural Information Processing Systems (NeurIPS) 2024 Reviewer for IEEE Spoken Language Technology Workshop (SLT) 2024 Member IEEE Signal Processing Society (SPS) Society Scholarships \u0026amp; Awards # Mitacs Globalink Research Internship by Mitacs, Canada in Mar 2015 Inspire Scholarship for Higher Education (S.H.E) : Issued by Department of Science and Technology (DST), Ministry of Science and Technology, Government of India in Aug 2011 National Talent Search Examination Scholar (NTSE) : Issued by National Council of Educational Research and Training in Mar 2009 ","date":"26 November 2024","permalink":"/research/","section":"Shefali Garg","summary":"","title":"Research"},{"content":"Topic 1: Transformers and Attention ‚Äì The Backbone of LLMs #Welcome to the first deep dive in the series! In this topic, we\u0026rsquo;ll explore Transformers and Attention, two concepts that form the core of modern Large Language Models (LLMs) like Gemini and GPT. But before we dive into this, it‚Äôs important to have a basic understanding of Machine Learning and Deep Learning concepts. If you\u0026rsquo;re new to these topics, don‚Äôt worry! You can check out these excellent courses to build foundational knowledge.\nCourse Recommendations\nMachine Learning Specialization (Stanford \u0026amp; DeepLearning.AI) | Coursera - A beginner-friendly, 3-course program by AI visionary Andrew Ng for fundamental AI and ML concepts. DeepLearning.AI‚Äôs Deep Learning Specialization on Coursera ‚Äì A comprehensive and Intermediate level- series to get you up to speed with the basics of deep learning with neural nets. What is Language Modeling? #Language modeling is the task of predicting the next word or sequence of words in a sentence based on the preceding context. It involves understanding the structure, grammar, and meaning of language, which enables a model to generate coherent and contextually appropriate text. Language models are trained on large corpora of text and learn the statistical relationships between words or tokens, which allows them to predict how likely a word is to appear next in a sequence.\nThe model generates text by considering previous words (or tokens) and using that context to generate the next word, phrase, or sentence. For example:\nContext: \u0026ldquo;The weather today is\u0026rdquo; Prediction: \u0026ldquo;sunny\u0026rdquo; (or any other word that fits based on the model‚Äôs training) Language models are widely used in many NLP tasks, Text Generation and Summarization, Machine Translation etc. Traditional language models, like n-gram models, used simple statistical methods to predict the next word based on the preceding ones. However, modern deep learning-based language models use more powerful neural networks to learn more complex patterns and relationships within the language.\nBefore Transformers, models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks were used for sequence processing. RNNs and LSTMs processed text one token at a time, maintaining a memory of previous tokens. However, they struggled with long-range dependencies, as their memory faded over long sequences, making it difficult to capture relationships in complex texts.\nReading Recommendations # Excellent Blog for understanding LSTMs: https://colah.github.io/posts/2015-08-Understanding-LSTMs/ Why LSTM/RNN Fail: https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0 Sequence to Sequence Models: Understanding Encoder-Decoder Sequence to Sequence Model | by Simeon Kostadinov | Towards Data Science Research Papers # [1409.3215] Sequence to Sequence Learning with Neural Networks The Transformer architecture solves this by utilizing attention mechanisms to enable parallel processing of data and capturing dependencies regardless of their distance in a sequence.\nWhat are Transformers And Attention ? #Transformers are the backbone of modern Natural Language Processing (NLP) and are the primary architecture behind LLMs. Introduced by Google in 2017 in the groundbreaking paper ‚ÄúAttention is All You Need‚Äù by Vaswani et al. (2017), Transformers revolutionized the way we process sequences of data, particularly for language modeling. The Transformer architecture was initially developed for translation tasks. It is a sequence-to-sequence model designed to convert sequences from one domain to another, such as translating French sentences into English.\nThe original Transformer architecture (shown in Figure 1) consists of two main components: the encoder and the decoder. The encoder processes the input text (e.g., a French sentence) and transforms it into a numerical representation. This representation is then passed to the decoder, which generates the output text (e.g., an English translation) in an autoregressive manner, meaning one word is predicted at a time based on the previously generated words.\nFigure 1 : Original Transformer Architecture (Ref: https://arxiv.org/abs/1706.03762)\nDon‚Äôt worry about the complex diagram above, and I don‚Äôt expect you to be familiar with terms like multi-head attention etc., just yet. :). I‚Äôll break down the key components of the Transformer for you and share the best resources to explore them in depth.\nThese are the key components of a transformer model.\nInput Embedding/Output Embedding:\nInput embeddings are used to represent the input tokens (e.g., words or subwords) provided to the model, while output embeddings represent the tokens predicted by the model. For instance, in a machine translation task, the input embeddings correspond to words in the source language, and the output embeddings correspond to words in the target language. Positional Encoding:\nSince Transformers don‚Äôt process data sequentially (like RNNs), positional encoding is used to add information about the position of words in a sentence so that the model can understand the order of words. Attention: This is the place where the magic happens !\nAttention allows a model to focus on the most relevant parts of the input while processing a sequence. This is crucial for understanding language because words can have relationships that extend over long distances. For example, in the sentence ‚ÄúThe cat that chased the mouse was very fast‚Äù, attention helps the model understand that ‚Äúcat‚Äù and ‚Äúwas‚Äù are related, even though they are far apart. The primary type of attention in Transformers is self-attention, which allows each word (or token) in a sequence to ‚Äúattend‚Äù to all the other words in the sequence when making predictions. This capability enables the Transformer to capture contextual relationships at scale.\nFigure 2 : One word \u0026ldquo;attends\u0026rdquo; to other words in the same sentence differently.\nRef: https://lilianweng.github.io/posts/2018-06-24-attention/ Multi-Head Attention: Instead of having a single attention mechanism, multi-head attention uses multiple attention layers to learn different relationships in the data simultaneously. This allows the model to capture more diverse patterns. Masked Multi-Head Attention: We force the model to only calculate attention corresponding to only the previous tokens that it has seen in the sentence and not for the future tokens. Used in the Decoder part of the transformer. Blogs explaining attention in detail:\nVisualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) ‚Äì Jay Alammar\nAttention? Attention! | Lil\u0026rsquo;Log\nFeed-Forward Layers:\nAfter the attention layer, a feed-forward network is applied to transform the representations for each token. This adds non-linearity and complexity to the model. Add \u0026amp; Norm Layers :\nEvery transformer layer consisting of a multi-head attention module and a feed-forward layer also employs layer normalization and residual connections. This is corresponding to the Add and Norm layer in Figure 1, where ‚ÄòAdd‚Äô corresponds to the residual connection and ‚ÄòNorm‚Äô corresponds to layer normalization. Output Layer:\nAn Output Layer, such as the Softmax layer, produces the final predictions of the model,converting the computed values into probabilities for tasks like classification or generating text. Reading Recommendations # I highly encourage this blog to anyone who wants to understand transformers and attention in the most simplified manner. It beautifully explains all the above concepts visually. The Illustrated Transformer ‚Äì Jay Alammar Youtube Video for the same: https://www.youtube.com/watch?v=-QH8fRhqFHM Transformer Paper: [1706.03762] Attention Is All You Need Hugging Face‚Äôs Course on Transformers ‚Äì Hands-on tutorials with practical implementation examples. Other References/Resources # Residual Networks: [1512.03385] Deep Residual Learning for Image Recognition Layer Normalization: [1607.06450] Layer Normalization Wrapping Up # Transformers revolutionized large language models (LLMs) and AI with the introduction of the self-attention mechanism, allowing models to effectively capture long-range dependencies and contextual relationships across entire sequences. Unlike RNNs and LSTMs, Transformers process data in parallel, significantly enhancing scalability and training speed. The architecture supports massive pretraining on diverse datasets, followed by fine-tuning for specific tasks, enabling powerful transfer learning capabilities. This scalability and flexibility have driven breakthroughs in performance, enabling the development of state-of-the-art LLMs like GPT and BERT. These models excel in a variety of tasks, including text generation, translation, and summarization. I hope you enjoyed the article and found the insights on transformers and their impact on AI and large language models helpful. If you have any questions or would like to explore the topic further, feel free to reach out!\nHappy Reading! :D\n","date":"25 November 2024","permalink":"/posts/tranformer/","section":"Posts","summary":"","title":"Transformers and Attention: The Backbone of LLMs"},{"content":"","date":null,"permalink":"/tags/self-supervised-training/","section":"Tags","summary":"","title":"Self-Supervised Training"},{"content":"","date":null,"permalink":"/tags/speech/","section":"Tags","summary":"","title":"Speech"},{"content":"Understanding BEST-RQ: A Simple Self-Supervised Pre-training Approach for Speech Recognition #Have you ever wished your voice assistant understood you a little better? Automatic speech recognition (ASR) technology is constantly evolving, but achieving high accuracy can be complex. Today, we‚Äôll explore a new approach called BEST-RQ (BERT-based Speech pre-Training with Random-projection Quantizer) introduced by Google Researchers in 2023 that has made waves in the field.\nThis innovative technique leverages the power of BERT, a popular language model, specifically adapted for speech recognition. But BEST-RQ takes it a step further by introducing a unique scalable pre-training method, allowing the model to learn effectively from vast amounts of unlabelled data, leading to significant improvements in accuracy.\nIntrigued? In this short blog post, we‚Äôll break down the key concepts behind BEST-RQ and how it‚Äôs transforming speech recognition. We‚Äôll explore how it simplifies the learning process and unlocks the potential for even more accurate and robust ASR systems in the future!\nProblem #Speech recognition aims to convert spoken language into written text. Traditional methods rely on labeled data, where spoken words are paired with their corresponding written text. But this approach requires a vast amount of labeled data, which can be expensive and time-consuming to collect.\nOne common design principle of self-supervised learning for speech recognition centers around learning representations. Inspired by the success of BERT (Devlin et al., 2018), a powerful technique in natural language processing, one research trend in the speech community is to build BERT-inspired algorithms.\nBERT is not directly applicable to speech because speech lacks these discrete tokens. We can‚Äôt directly feed the continuous speech signal into BERT, hence a need to bridge the gap between continuous speech signals and the discrete text tokens, and a solution for addressing this issue is through learning speech representation.\nWhile speech representation learning is crucial, integrating it with self-supervised learning presents two challenges.\nModel Architecture Limitation\nThe model must excel at both representation and downstream tasks, but optimal representation learning might not translate to efficient downstream processing (e.g., accessing future context for representation vs. low-latency requirements for recognition). Increased Design Complexity\nThe misaligned objectives and intricate design process for these combined algorithms can hinder research progress, potentially favoring complex solutions over simpler alternatives. The core problem BEST-RQ tackles is: how can we train a speech recognition model effectively without relying on a large amount of labeled data or needing to break speech down into discrete units like words, and at the same time keeping it all ‚ÄúSIMPLE‚Äù to be able to scale to larger new speech model architectures?\nOccam‚Äôs Razor for ML: Simplest solution often wins [Ref. Image]\nBEST-RQ‚Äôs Solution #BEST-RQ tackles the above problem by offering a compelling alternative. It introduces a novel technique of self-supervised training using a combination of Random Projection Quantizer (RPQ) and Masked Language Modeling (MLM).\nFig 1: Overview of BEST-RQ. The approach applies random projections to project the input speech signals to a randomly initialized codebook, and map them to discrete labels through finding the nearest vector in the codebook. The pre-training objective is for the ASR encoder to take the masked input signals and predict the labels corresponding to the masked part provided by the random-projection quantizer. Figure taken from Ref. [1]\nRandom Projection Quantizer (RPQ) #This is the heart of BEST-RQ and is the core innovation that bridges the gap between continuous speech and the discrete world BERT thrives in. RPQ has two key components - Projection matrix and Codebook both randomly initialized and not updated during training.\nProjection matrix #This projects the speech features (numerical representation of speech) into a lower dimension. The framework is described in Figure 1. This matrix is of size d x k, where:\nd is the dimensionality of the original speech features (typically high, like hundreds or thousands). k is the target dimensionality after projection (usually much lower than d). Codebook # To put simply, this is a collection of n code vectors, each of size k. These vectors represent the discrete code space. The size n of the codebook is a hyper parameter that can be tuned based on the specific task and dataset. The projection matrix A use Xavier initialization (Glorot \u0026amp; Bengio, 2010) and the codebook C use standard normal distribution for initialization, and the parameters are fixed during the pre-training process and therefore the quantizations are consistent during training.\nGiven an input vector x where x is a d-dimensional vector computed from speech signals, the random-projection quantizer maps x to discrete labels y through\nwhere:\n* A denotes a randomly initialized h √ó d matrix\n* C = {c1, ‚Ä¶, cn} is a set of randomly initialized hi-dimensional vectors\n* norml2() is a function that normalize the vector to have unit L2 norm.\nThe input data is normalized to have 0 mean and standard deviation of 1. The normalization is critical for pre- Self-supervised Learning with Random-projection Quantizer for Speech Recognition preventing the random projection to collapse to a small subset of codes.\nAs shown in Figure 1, The BEST- RQ algorithm masks speech signals (explained below) and feeds them to the encoder part of the speech recognition model.\nThe encoder learns to predict the masked region based on the unmasked speech signals where the learning targets are labels provided by RPQ.\nThe RPQ projects speech signals to a randomly initialized matrix, and finds a nearest vector in a randomly initialized codebook. The index of that vector is the target label.\nMasked Language Modeling (MLM)\nSimilar to how BERT works with text, BEST-RQ uses MLM for training. Speech segments are masked (replaced with silence or noise).\nThe approach applies masks directly on the speech signal, where the masking strategy samples at every frame whether to apply masks with a fixed probability. Each mask spans from the starting frame with a fixed length. The masked parts are replaced with a noise sampled from a normal distribution with 0 mean and 0.1 standard deviation.\nThe model, typically a Transformer architecture, is tasked with predicting the masked parts based on the surrounding context. Instead of predicting words like BERT, the model predicts the labels (codebook indices) of the masked speech using the RPQ predictions as targets.\nThe pre-training process adds a softmax layer on top of the ASR encoder to learn to predict the quantized speech labels.\nKey Point: Frozen and Independent RPQ\nUnlike other self-supervised methods, RPQ (projection matrix and codebook) is randomly initialized and not trained during the process. This removes the need for the model to learn the intricacies of the codebook, allowing it to focus solely on capturing meaningful speech representations.\nSince the random-projection quantizer is independent of the ASR encoder, the pre-training is flexible and can work with different architectures of the ASR encoder.\nBenefits of this approach # Performance\nBEST-RQ achieves competitive results compared to other methods, even with lower latency for real-time applications.\nThis approach shows similar WERs as the existing state-of-the-art results on LibriSpeech with non-streaming models, and outperform wav2vec 2.0 and w2vBERT on LibriSpeech with streaming models and on multilingual tasks with non-streaming models.\nOn multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT. Flexibility\nThe model architecture is independent of the RPQ design. Neither the matrix nor the codebook is updated during self-supervised learning. Since the random-projection quantizer is not trained and\nis separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. Simplicity and Focus on Core Task\nBy avoiding complex representation learning, BEST-RQ spends less time grappling with representation learning and can concentrate on the core task ‚Äî predicting the masked parts of the speech using the provided codebook labels. Here‚Äôs an analogy: Imagine you‚Äôre lost in a giant forest (high-dimensional speech data) and need to find a specific landmark (desired speech representation).\nTraditional methods: You meticulously learn the entire forest layout (complex representation learning) to navigate and find the landmark.\nBEST-RQ approach: You‚Äôre given a helicopter (random projection matrix) that takes you high above the forest (lower dimension) and a pre-defined map (codebook) with landmarks marked. You simply find the location that looks most similar to your current view (nearest codebook vector) ‚Äî faster and with less effort!\nFurther Exploration # Research is ongoing to understand how well RPQ captures speech information compared to learned quantizers. Exploring different types of quantizers and experiment designs might yield further improvements. Thank you very much for reading this and I hope I was able to clarify a few notions to the people who are just starting to get into Speech Models Pre-training World. Please feel free to suggest edits if you find any mistakes !\nReferences # Self-Supervised Learning with Random-Projection Quantizer for Speech Recognition by Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu \u0026amp; Yonghui Wu. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. Attention Is All You Need by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin Understanding the difficulty of training deep feedforward neural networks by Xavier Glorot \u0026amp; Yoshua Bengio wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training, Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, Yonghui Wu ","date":"6 April 2024","permalink":"/posts/bestrq/","section":"Posts","summary":"","title":"Understanding BEST-RQ: A Simple Self-Supervised Pre-training Approach for Speech Recognition"},{"content":" Title: The Answer To Everything :)Medium: Acrylic\nTitle: Wear Your CrownMedium: Acrylic\nTitle: The Beginnings With Lord GaneshaMedium: Acrylic\nTitle: It Doesn‚Äôt Always Have To Make Sense!Medium: Acrylic\nTitle: DreamMedium: Ink\nTitle: FriendsMedium: Acrylic\nTitle: The GirlMedium: Acrylic\nTitle: The FirstMedium: Charcoal\nTitle: WalkMedium: Acrylic\n","date":"1 January 2024","permalink":"/meta/art/","section":"Meta","summary":"","title":"Let's Paint a Little :)"},{"content":"","date":null,"permalink":"/meta/","section":"Meta","summary":"","title":"Meta"}]